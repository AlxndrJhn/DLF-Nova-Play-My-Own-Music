{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://datascience.stackexchange.com/questions/55566/tool-for-labeling-audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "\n",
    "import simple_model_feature\n",
    "dataset_path = Path('datasets')\n",
    "\n",
    "dataset_id = 1\n",
    "chunk_length = 10 # seconds\n",
    "chunk_move_step = 10 # seconds\n",
    "\n",
    "path_labels = dataset_path / f'{dataset_id:02}.txt'\n",
    "path_mp3 = dataset_path / f'{dataset_id:02}.mp3'\n",
    "chunk_path = dataset_path / f'{dataset_id:02}_split'\n",
    "labels = pandas.read_csv(path_labels, sep='\\t', header=None,\n",
    "                        names=['start', 'end', 'annotation'],\n",
    "                        dtype=dict(start=float,end=float,annotation=str))\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from simple_model import load_trained_model\n",
    "\n",
    "model = load_trained_model('model_saves/weights.best.basic_cnn.hdf5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with validation dataset\n",
    "import pandas as pd\n",
    "featuresdf = pd.read_pickle('data_chunks_df.pickle')\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Model accuracy: %.4f%%\" % accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sound = AudioSegment.from_mp3(path_mp3)\n",
    "# print(f'The file {path_mp3} is {len(sound)/1000/60:.1f}min long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # intervals\n",
    "# intervals = []\n",
    "# rows = list(labels.iterrows())\n",
    "# for r, next_r in zip(rows[:-1], rows[1:]):\n",
    "#     intervals.append([r[1]['start'], next_r[1]['start'], r[1]['annotation']])\n",
    "# intervals.append([rows[-1][1]['start'], len(sound)/1000, rows[-1][1]['annotation']])\n",
    "# print('\\n'.join([str(x) for x in intervals]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from numpy import linspace\n",
    "# import shutil\n",
    "# import os\n",
    "# from tqdm import tqdm\n",
    "# import io\n",
    "# import importlib\n",
    "# importlib.reload(feature)\n",
    "\n",
    "# classifications = []\n",
    "# for start, end, label in tqdm(intervals):\n",
    "#     chunk_start = start\n",
    "#     while chunk_start+chunk_length < end:\n",
    "#         sound_chunk = sound[int(chunk_start*1000):int((chunk_start+chunk_length)*1000)]\n",
    "#         sound_chunk = sound_chunk.set_frame_rate(44100)\n",
    "\n",
    "#         channels = 2\n",
    "#         samples = [float(x) for x in sound_chunk.get_array_of_samples()]\n",
    "#         stacked = np.vstack((samples[0::channels], samples[1::channels]))\n",
    "#         data = feature.extract_features(stacked, sound_chunk.frame_rate)\n",
    "\n",
    "#         prediction = model.predict_classes(data.reshape(1,80))[0]\n",
    "#         if prediction == 0:\n",
    "#             prediction_label = 'm'\n",
    "#         else:\n",
    "#             prediction_label = 'p'\n",
    "\n",
    "#         classifications.append(prediction_label == label)\n",
    "#         chunk_start += chunk_move_step\n",
    "#     print(f'\\n{sum(classifications)/len(classifications)*100:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online\n",
    "def classify_sound_chunk(sound_chunk):\n",
    "    channels = 2\n",
    "    samples = [float(x) for x in sound_chunk.get_array_of_samples()]\n",
    "    stacked = np.vstack((samples[0::channels], samples[1::channels]))\n",
    "    data = feature.extract_features(stacked, sound_chunk.frame_rate)\n",
    "\n",
    "    prediction = model.predict_classes(data.reshape(1,80))[0]\n",
    "    if prediction == 0:\n",
    "        prediction_label = 'm'\n",
    "    else:\n",
    "        prediction_label = 'p'\n",
    "    return prediction_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from IPython.display import display, clear_output\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://dradio-edge-209a-fra-lg-cdn.cast.addradio.de/dradio/nova/live/mp3/128/stream.mp3\" # nova\n",
    "#url = \"http://st01.dlf.de/dlf/01/128/mp3/stream.mp3\" # DLF\n",
    "#url = \"https://st02.sslstream.dlf.de/dlf/02/128/mp3/stream.mp3\" # kultur\n",
    "\n",
    "u = urlopen(url)\n",
    "buffer = []\n",
    "mute_states = []\n",
    "mute_state_n = 8\n",
    "is_online = False\n",
    "while True:\n",
    "    data = u.read(1024*2)\n",
    "    buffer.append(data)\n",
    "    concat = b''.join(buffer)\n",
    "\n",
    "    try:\n",
    "        buffer_as_audiosegment = AudioSegment.from_mp3(BytesIO(concat))\n",
    "    except:\n",
    "        print('error')\n",
    "        buffer = []\n",
    "        u = urlopen(url)\n",
    "        is_online = False\n",
    "        continue\n",
    "\n",
    "    if len(buffer_as_audiosegment) >= chunk_length*1000:\n",
    "        cropped = buffer_as_audiosegment[:10000]\n",
    "        classification = classify_sound_chunk(cropped)\n",
    "\n",
    "        mute_states.append(classification == 'p')\n",
    "        if len(mute_states) > mute_state_n:\n",
    "            mute_states = mute_states[len(mute_states)-mute_state_n:]\n",
    "        mute = sum(mute_states) / len(mute_states)\n",
    "        mapping = {'p':'person/news','m':'music'}\n",
    "        clear_output(wait=True)\n",
    "        display(f' {datetime.now()} Classification: {mapping[classification]:>15}, sound state: {mute*100: 3.0f} %')\n",
    "        buffer = buffer[3:]\n",
    "        is_online = True\n",
    "    else:\n",
    "        if not is_online:\n",
    "            clear_output(wait=True)\n",
    "            missing = chunk_length - len(buffer_as_audiosegment) / 1000\n",
    "            display(f'{datetime.now()} Loading additional {missing: 4.1f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bitvenvvenv35ee53c699b844df969397a76fdc5b59",
   "display_name": "Python 3.7.5 64-bit ('.venv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}